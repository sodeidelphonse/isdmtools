% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Metrics.R
\name{compute_metrics}
\alias{compute_metrics}
\title{Compute Evaluation Metrics for Integrated Spatial Models from Multisource Datasets}
\usage{
compute_metrics(
  test.data,
  prob.raster = NULL,
  xy.excluded = NULL,
  expected.response = NULL,
  n.background = 1000,
  responseCounts = "counts",
  responsePA = "present",
  threshold.method = c("best", "fixed"),
  best.method = c("youden", "closest.topleft"),
  fixed.threshold = NA_real_,
  best.threshold.policy = c("first", "last", "max.prec", "max.recall", "max.accu",
    "max.f1"),
  metrics = NULL,
  overall.roc.metrics = NULL,
  overall.error.metrics = NULL,
  is.pred.rate = FALSE,
  exposure = NULL,
  seed = 25,
  ...
)
}
\arguments{
\item{test.data}{A named \code{list} of \code{sf} objects. Each \code{sf} object represents a different test dataset and must contain point geometries. The function will loop through each named dataset in the list. In particular, \code{test.data} can be a 'fold' from the \link{create_folds} and \link{extract_fold} outputs, if independent validation datasets are not available.}

\item{prob.raster}{A \code{SpatRaster} object with unique layer containing the model's predictions on a probability scale (0-1). It represents a suitability index, and its values are used to compute all ROC-based metrics (e.g., AUC, TSS, F1 score). This argument is optional if only continuous-outcome metrics are requested for count data.}

\item{xy.excluded}{An optional \code{SpatVector} or \code{sf} object representing locations where pseudo-absence points should not be sampled, such as occupied areas or known background points. Only relevant for presence-only (PO) data. Default is \code{NULL}.}

\item{expected.response}{A \code{SpatRaster} object containing the model's predictions on a continuous scale (i.e. counts or rate if offset is used; see \link{suitability_index}). Its values are used to compute all continuous-outcome metrics (e.g., RMSE, MAE, MAPE). This argument is required if a continuous-outcome metric is requested.}

\item{n.background}{integer. It specifies the number of pseudo-absence points to sample for presence-only data. Default is 1000 (see \link{sample_background}).}

\item{responseCounts}{character. The column name in the \code{sf} objects that contains observed counts. Default is 'counts' and must be standardized across all count data sets. Exceptionally, positive measurements (e.g. biomass) are supported by allowing exposure to its default value. In such cases, only continuous-outcome metrics can be requested.}

\item{responsePA}{character. The column name in the \code{sf} objects that contains presence-absence data (1 for presence, 0 for absence). Default is 'present' and must be standardized across all PA data sets.}

\item{threshold.method}{character. The method to be used for selecting the threshold for converting probabilities to binary outcomes. Options are 'best' (using \code{best.method}) or 'fixed'. Default is "best".}

\item{best.method}{character. The method to be used for selecting the best threshold when \code{threshold.method} is 'best'. Options are 'youden' or 'closest.topleft'. Default is "youden" criterion which maximizes the sensitivity and specificity.}

\item{fixed.threshold}{numeric. The value (0-1) to be used as the fixed threshold when \code{threshold.method} is 'fixed'. Default is \code{NA_real_}.}

\item{best.threshold.policy}{character. Specifies the policy for selecting a threshold when multiple thresholds yield the same 'best' value. Options are "first", "last", "max.prec" (max precision), "max.recall" (max recall), "max.accu" (max accuracy), or "max.f1" (max F1 score). Default is "first".}

\item{metrics}{character. A vector of the metric names to compute. If \code{NULL}, "auc" (area under the ROC curve), "tss" (true skill statistics), "accuracy", "F1" (F1 score), "precision", and "recall" are computed for ROC-based metrics while "rmse" (root mean squared error), "mae" (mean absolute error) and "r2" (pseudo R-squared) are computed for error-based metrics.}

\item{overall.roc.metrics}{character. A vector of a subset of ROC-based metrics to be used for the overall composite score (\code{TOT_ROC_SCORE}). Allowed options are "auc", "tss", "accuracy", and "F1". If \code{NULL}, the sensible default is "auc", "tss" and "accuracy".
This metric is useful when the objective is to obtain a rapid overview of the rank of multiple candidate models fitted to datasets via blocked cross-validation using multi-criteria assessment.}

\item{overall.error.metrics}{character. A vector of a subset of continuous outcome metrics to be used for the overall composite score (\code{TOT_ERROR_SCORE}). Allowed options are "rmse", "mae", and "r2".
If \code{NULL}, the default is "rmse" and "mae". In order to obtain an overall interpretable score, it is imperative to select metrics that have the same scale.}

\item{is.pred.rate}{logical. If \code{TRUE}, it indicates that the \code{expected.response} contains predictions at the intensity (per-unit-of-exposure) scale (typical for Bayesian models with offset from \code{inlabru}). If \code{FALSE}, it assumes predictions are at the original scale (e.g., counts). Default is \code{FALSE}.}

\item{exposure}{character. The column name in the \code{sf} objects that contains the exposure variable (offset). Only relevant for count (and sometimes presence-absence) data and must be standardized across all these types of datasets.
If \code{is.pred.rate} is \code{TRUE}, observed counts are rescaled by this exposure variable. Default is \code{NULL}.}

\item{seed}{integer. It sets the seed for random number generation, used for pseudo-absence sampling to ensure reproducibility. Default is 25.}

\item{...}{Additional arguments to be passed on to internal functions, particularly \link[pROC]{coords} function.}
}
\value{
A named \code{list} containing all requested metrics. The names follow a consistent convention:
\itemize{
\item \code{"<METRIC>_<DATASET_NAME>"}: Individual metric score for each dataset.
\item \code{"<METRIC>_Comp"}: The sample-size-weighted composite score for a given metric across all valid datasets.
\item \code{"TOT_ROC_SCORE"}: The overall ROC-based composite score, averaged across the selected \code{overall.roc.metrics}.
\item \code{"TOT_ERROR_SCORE"}: The overall error-based composite score, averaged across \code{overall.error.metrics}.
}
}
\description{
This function computes a wide range of evaluation metrics for a single-layer raster of model predictions against a list of one or more point-based datasets. It is designed to handle different data types (presence-only, presence-absence, and count data) and provides individual metrics as well as dataset-weighted composite scores.
}
\details{
The function handles three main data types and any combination thereof:

\itemize{
\item \strong{Presence-Absence (PA) Data:} The function uses the \code{responsePA} column and \code{prob.raster} to calculate all ROC-based metrics (see \link[pROC]{coords}, for more details on available metrics).
\item \strong{Count Data (or optionally measurements):} The function uses \code{expected.response} to calculate continuous-outcome metrics and can optionally use \code{prob.raster} to calculate ROC-based metrics for count data.
\item \strong{Presence-Only (PO) Data:} The function uses the presence points from the \code{sf} object (\code{xy.excluded}) and samples \code{n} pseudo-absence points from the study background (excluding \code{xy.excluded}) to create a presence-absence dataset for ROC-based metric calculations.
}

For models based on count data, if a user wants to compute both continuous-outcome and ROC-based metrics, \code{expected.response} raster must be supplied for the continuous metrics and \code{prob.raster} must also be supplied for the ROC-based metrics.
The \code{prob.raster} can be obtained by converting the continuous-outcome prediction (e.g., \verb{linear predictor}) to a suitability index using the \link{suitability_index} function.

The available continuous-outcome metrics are given as follows:
\itemize{
\item \strong{Root Mean Squared Error (RMSE)}: A measure of the average magnitude of the errors. It's the square root of the average of squared differences between prediction and actual observation. It gives higher weight to large errors.
\deqn{RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2}}.
\item \strong{Mean Absolute Error (MAE)}: A measure of the average magnitude of the errors without considering their direction. It is the average of the absolute differences between prediction and actual observation.
\deqn{MAE = \frac{1}{n}\sum_{i=1}^{n}|\hat{y_i} - y_i|}.
\item \strong{Mean Absolute Percentage Error (MAPE)}: A measure of prediction accuracy as a percentage. It is calculated as the average of the absolute percentage errors for each observation. It can be useful for comparing performance across different datasets or models.
\deqn{MAPE = \frac{100\%}{n}\sum_{i=1}^{n}|\frac{\hat{y_i} - y_i}{y_i}|}.
\item \strong{Pseudo R-squared (\eqn{R^2})}: A measure of the proportion of variance in the observed data explained by the model's predictions.
\deqn{R^2 = 1 - \frac{SS_{res}}{SS_{tot}}}
Where:
\itemize{
\item \eqn{y_i} is the observed continuous value at location \eqn{i}.
\item \eqn{\hat{y}_i} is the predicted value from the model at location \eqn{i} (e.g., the posterior mean of the predictions).
\item \eqn{\bar{y}} is the mean of all observed values.
\item \eqn{SS_{res}} is the residual sum of squares, which measures the discrepancy between the observed and predicted values:
\deqn{SS_{res} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\item \eqn{SS_{tot}} is the total sum of squares, which measures the total variance in the observed data:
\deqn{SS_{tot} = \sum_{i=1}^{n}(y_i - \bar{y})^2}
}
}

A \verb{weighted composite score} (\verb{<METRIC>_Comp}) is computed for each requested metric by taking the sample-size-weighted average across all datasets where the metric was successfully calculated.
A \verb{total composite score} (\code{TOT_ROC_SCORE} or \code{TOT_ERROR_SCORE}) is also computed by averaging the selected metrics in the corresponding \verb{overall metrics} character vector.
It can be viewed as a quick \emph{multi-criterion decision metric} for multiple models comparison.
}
\examples{
\dontrun{
# Assuming you have dummy rasters and a list of sf objects
# with 'counts' and 'present' columns.

# Example 1: Compute metrics for a presence-absence model
# my_metrics <- compute_metrics(
#   test.data = list(ds1 = my_pa_sf),
#   prob.raster = my_prob_raster  # compulsory prob.raster
# )

# Example 2: Compute continuous-outcome metrics for a count-based model
# cont_metrics <- compute_metrics(
#   test.data = list(ds1 = my_count_sf),
#   expected.response = expected_raster, # prediction raster on count scale
#   metrics = c("rmse", "mae", "mape")
# )

# Example 3: Compute both continuous and ROC-based metrics for a count model
# The user must first generate a suitability index (prob_raster)
# from the linear scale prediction (pred_eta).
# expected_raster <- suitability_index(pred_eta, response.type = "count",
#                    output.format = "response")
# suitability_raster <- suitability_index(pred_eta, response.type = "count",
#                        output.format = "prob")
# full_metrics <- compute_metrics(
#   test.data = list(ds1 = my_count_sf),
#   prob.raster = suitability_raster,
#   expected.response = expected_raster,
#   metrics = c("rmse", "mae", "auc", "tss")
# )

# Example 4: Handle an inlabru-like model with an offset term
# The `expected.response` raster is at the intensity scale (rate).
# cont_metrics <- compute_metrics(
#   test.data = list(ds1 = my_count_sf),
#   prob.raster = suitability_raster,
#   expected.response = expected_raster,
#   metrics = c("rmse", "auc", "tss"),
#   is.pred.rate = TRUE,
#   exposure = "exposure_col"
# )

# Example 5: Compute dataset-specific and weighted composite metrics for a joint model
# expected_raster  <- suitability_index(pred_eta, response.type = "count.pa",
#                  output.format = "response")
# suitability_raster <- suitability_index(pred_eta,
#                    response.type = "count.pa", has.offset = FALSE)
# full_metrics <- compute_metrics(
#   test.data = list(ds1 = my_count_sf, ds2 = my_pa_sf),
#   prob.raster = suitability_raster,
#   expected.response = expected_raster,
#   metrics = c("rmse", "mae", "auc", "tss", "accuracy")
# )
}
}
\seealso{
\code{\link{extract_fold}}, \code{\link{sample_background}}

Other ISDM evaluation methods: 
\code{\link{print.ISDMmetrics}()}
}
\concept{ISDM evaluation methods}
