---
title: "Get started"
date: "Generated on `r Sys.Date()`"
author: "Akoeugnigan Idelphonse SODE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE
)
```

```{r warning=FALSE,message=FALSE}
library(isdmtools)
library(sf)
library(terra)
library(dplyr)
library(ggplot2)
```

# Introduction

This vignette should bridge the gap between "having spatial data" and "being ready for modelling." The fundamental philosophy of `isdmtools` is to provide a standardised bridge between diverse biodiversity spatial data sources and a robust spatial cross-validation (CV) strategy for evaluating integrated species distribution models (ISDMs). This tutorial will assist you through preparing data and generating spatial folds â€” the essential first steps before fitting an Integrated Species Distribution Model (ISDM).

# Data Preparation

## Step 1: Simulate some spatial data

Let's consider a simple scenario by generating two spatial datasets representing the presence and abundance of a given species over the defined study area. 

```{r }
# Simulate a list of presence-only and count data
set.seed(42)
presence_data <- data.frame(
  x = runif(100, 0, 4), 
  y = runif(100, 6, 13), 
  site = rbinom(100, 1, 0.6)
) %>% st_as_sf(coords = c("x", "y"), crs = 4326)

count_data <- data.frame(
  x = runif(50, 0, 4), 
  y = runif(50, 6, 13), 
  count = rpois(50, 5)
) %>% st_as_sf(coords = c("x", "y"), crs = 4326)

datasets_list <- list(Presence = presence_data, Count = count_data)

# Define the study region (e.g. Benin's boundary rectangle)
ben_coords <- matrix(c(0, 6, 4, 6, 4, 13, 0, 13, 0, 6), ncol = 2, byrow = TRUE)
ben_sf <- st_sf(data.frame(name = "Region"), 
                          st_sfc(st_polygon(list(ben_coords)), 
                          crs = 4326))
                          
# Gnerate some continuous covariates
set.seed(42)
r   <- rast(ben_sf, nrow = 100, ncol = 100)
r[] <- rnorm(ncell(r))

rtmp   <- r
rtmp[] <- runif(ncell(r), 5, 10)

r <- c(r, rtmp + r)
names(r) <- c("cov1", "cov2")
```

## Step 2: Spatial Partitioning

Partitioning spatial data into spatial folds is important since it helps reduce spatial autocorrelation in the observations and estimate a more realistic model performance.

```{r }
# Create the DataFolds object using the default method
folds <- create_folds(datasets_list, ben_sf, cv_method = "cluster")
```

```{r fold-plot, fig.width = 6, fig.height = 6, out.height= "100%", fig.align = "center"}
# Visualize the folds with custom styling
plot(folds, nrow = 2, annotate = FALSE) +
  scale_x_continuous(breaks = seq(0, 4, 1)) +
  scale_y_continuous(breaks = seq(6, 13, 2)) +
  theme_minimal() +
  labs(title = "Spatial Block Partitioning")
```

```{r auto-plot, fig.width = 4, fig.height = 4, out.width = "100%", fig.align = "center"}
# Create the DataFolds object using the `spatialsample` blocking engine
fold_ss <- create_folds(datasets_list, ben_sf, cv_method = "block")

# Using the native autoplot of `spatialesample`
ggplot2::autoplot(fold_ss)
```

```{r }
# Folds summary
summary(fold_ss)
```

## Step 3: Folds diagnostics

Various diagnostic analyses can be performed on the folds created in the previous step. Specifically, when there is prior information on the spatial range from an exploratory analysis, this value can be used as the `rho` argument to the `check_folds` function. This can then be used to compare to the estimated internal size and inter-block gap. The same procedure is applied to post-modelling analysis to check if the posterior range estimated aligns with the spatial geometry of the specified folds or blocks.

```{r, fig.width = 5, fig.height = 5, out.width = "90%"}
# Check spatial independence of folds
geo_diag <- check_folds(folds, plot = TRUE)
print(geo_diag)

# Plot results
plot(geo_diag) 

```

```{r warning=FALSE,message=FALSE}
# Check environmental balance of folds
set.seed(42)  # for background sample reproducibility
env_diag <- check_env_balance(folds,
                              covariates = r,
                              n_background = 5000
                              )
print(env_diag)

```

```{r diagnostics, fig.width = 6, fig.height = 4, out.width = "100%"}
# Plot outputs
plot(env_diag)

```

```{r }
# Combined diagnostics
summarise_fold_diagnostics(geo_diag, env_diag)

```

## Step 4: Data Extraction for Modelling

Once spatial folds are created, one can extract the data and see how it looks like right 
before it goes into a modelling tool like inlabru. You can access both 'train' and 'test' sets and their corresponding datasets as follows:

```{r }
# Extract fold 1
splits_1 <- extract_fold(folds, fold = 1)

# Accessing the training and testing sets for the "Presence" source
head(splits_1$train$Presence)

head(splits_1$test$Presence)
```

# Conclusion

Congratulations! You have successfully fused multi-source biodiversity data and generated spatially independent partitions for robust model validation. By using `create_folds()` and related folds' diagnostic tools, you've ensured that your model evaluation will account for spatial autocorrelation, providing a more realistic estimate of predictive performance. 

The `isdmtools` journey continues with model fitting and comprehensive evaluation. Depending on your needs, we recommend the following paths:

- *Model Fitting*: Use the training data extracted via `extract_fold()` to fit your models using modelling engines like inlabru, PointedSDMs, or standard GAMs tools which can support spatial data with multiple response formats.
- *Integrated Evaluation*: One predictions are obtained, evaluate the models and analyse the outputs.

The advanced guide on [ISDM Evaluation Workflow](isdm-workflow.html) covers model building with external tools, calculation of dataset-specific and composite scores, suitability analysis and mapping with `isdmtools`.

