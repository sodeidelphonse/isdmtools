<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Compute Evaluation Metrics for Integrated Spatial Models from Multisource Datasets — compute_metrics • isdmtools</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Compute Evaluation Metrics for Integrated Spatial Models from Multisource Datasets — compute_metrics"><meta name="description" content="This function computes a wide range of evaluation metrics for a single-layer raster of model predictions against a list of one or more point-based datasets. It is designed to handle different data types (presence-only, presence-absence, and count data) and provides individual metrics as well as dataset-weighted composite scores."><meta property="og:description" content="This function computes a wide range of evaluation metrics for a single-layer raster of model predictions against a list of one or more point-based datasets. It is designed to handle different data types (presence-only, presence-absence, and count data) and provides individual metrics as well as dataset-weighted composite scores."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">isdmtools</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/isdmtools.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/isdm-evaluation-workflow.html">ISDM Evaluation Workflow</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/sodeidelphonse/isdmtools/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Compute Evaluation Metrics for Integrated Spatial Models from Multisource Datasets</h1>
      <small class="dont-index">Source: <a href="https://github.com/sodeidelphonse/isdmtools/blob/main/R/Metrics.R" class="external-link"><code>R/Metrics.R</code></a></small>
      <div class="d-none name"><code>compute_metrics.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function computes a wide range of evaluation metrics for a single-layer raster of model predictions against a list of one or more point-based datasets. It is designed to handle different data types (presence-only, presence-absence, and count data) and provides individual metrics as well as dataset-weighted composite scores.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">compute_metrics</span><span class="op">(</span></span>
<span>  <span class="va">test_data</span>,</span>
<span>  prob_raster <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  xy_excluded <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  expected_response <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  n_background <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  response_counts <span class="op">=</span> <span class="st">"counts"</span>,</span>
<span>  response_pa <span class="op">=</span> <span class="st">"present"</span>,</span>
<span>  threshold_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"best"</span>, <span class="st">"fixed"</span><span class="op">)</span>,</span>
<span>  best_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"youden"</span>, <span class="st">"closest.topleft"</span><span class="op">)</span>,</span>
<span>  fixed_threshold <span class="op">=</span> <span class="cn">NA_real_</span>,</span>
<span>  best_threshold_policy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"first"</span>, <span class="st">"last"</span>, <span class="st">"max.prec"</span>, <span class="st">"max.recall"</span>, <span class="st">"max.accu"</span>,</span>
<span>    <span class="st">"max.f1"</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  overall_roc_metrics <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  overall_error_metrics <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  is_pred_rate <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  exposure <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">25</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-test-data">test_data<a class="anchor" aria-label="anchor" href="#arg-test-data"></a></dt>
<dd><p>A named <code>list</code> of <code>sf</code> objects. Each <code>sf</code> object represents a different test dataset and must contain point geometries. The function will loop through each named dataset in the list. In particular, <code>test_data</code> can be a 'fold' from the <a href="create_folds.html">create_folds</a> and <a href="DataFolds-methods.html">extract_fold</a> outputs, if independent validation datasets are not available.</p></dd>


<dt id="arg-prob-raster">prob_raster<a class="anchor" aria-label="anchor" href="#arg-prob-raster"></a></dt>
<dd><p>A <code>SpatRaster</code> object with unique layer containing the model's predictions on a probability scale (0-1). It represents a suitability index, and its values are used to compute all ROC-based metrics (e.g., AUC, TSS, F1 score). This argument is optional if only continuous-outcome metrics are requested for count data.</p></dd>


<dt id="arg-xy-excluded">xy_excluded<a class="anchor" aria-label="anchor" href="#arg-xy-excluded"></a></dt>
<dd><p>An optional <code>SpatVector</code> or <code>sf</code> object representing locations where pseudo-absence points should not be sampled, such as occupied areas or known background points. Only relevant for presence-only (PO) data. Default is <code>NULL</code>.</p></dd>


<dt id="arg-expected-response">expected_response<a class="anchor" aria-label="anchor" href="#arg-expected-response"></a></dt>
<dd><p>A <code>SpatRaster</code> object containing the model's predictions on a continuous scale (i.e. counts or rate if offset is used; see <a href="suitability_index.html">suitability_index</a>). Its values are used to compute all continuous-outcome metrics (e.g., RMSE, MAE, MAPE). This argument is required if a continuous-outcome metric is requested.</p></dd>


<dt id="arg-n-background">n_background<a class="anchor" aria-label="anchor" href="#arg-n-background"></a></dt>
<dd><p>integer. It specifies the number of pseudo-absence points to sample for presence-only data. Default is 1000 (see <a href="sample_background.html">sample_background</a>).</p></dd>


<dt id="arg-response-counts">response_counts<a class="anchor" aria-label="anchor" href="#arg-response-counts"></a></dt>
<dd><p>character. The column name in the <code>sf</code> objects that contains observed counts. Default is 'counts' and must be standardized across all count data sets. Exceptionally, positive measurements (e.g. biomass) are supported by allowing exposure to its default value. In such cases, only continuous-outcome metrics can be requested.</p></dd>


<dt id="arg-response-pa">response_pa<a class="anchor" aria-label="anchor" href="#arg-response-pa"></a></dt>
<dd><p>character. The column name in the <code>sf</code> objects that contains presence-absence data (1 for presence, 0 for absence). Default is 'present' and must be standardized across all PA data sets.</p></dd>


<dt id="arg-threshold-method">threshold_method<a class="anchor" aria-label="anchor" href="#arg-threshold-method"></a></dt>
<dd><p>character. The method to be used for selecting the threshold for converting probabilities to binary outcomes. Options are 'best' (using <code>best_method</code>) or 'fixed'. Default is "best".</p></dd>


<dt id="arg-best-method">best_method<a class="anchor" aria-label="anchor" href="#arg-best-method"></a></dt>
<dd><p>character. The method to be used for selecting the best threshold when <code>threshold_method</code> is 'best'. Options are 'youden' or 'closest.topleft'. Default is "youden" criterion which maximizes the sensitivity and specificity.</p></dd>


<dt id="arg-fixed-threshold">fixed_threshold<a class="anchor" aria-label="anchor" href="#arg-fixed-threshold"></a></dt>
<dd><p>numeric. The value (0-1) to be used as the fixed threshold when <code>threshold_method</code> is 'fixed'. Default is <code>NA_real_</code>.</p></dd>


<dt id="arg-best-threshold-policy">best_threshold_policy<a class="anchor" aria-label="anchor" href="#arg-best-threshold-policy"></a></dt>
<dd><p>character. Specifies the policy for selecting a threshold when multiple thresholds yield the same 'best' value. Options are "first", "last", "max.prec" (max precision), "max.recall" (max recall), "max.accu" (max accuracy), or "max.f1" (max F1 score). Default is "first".</p></dd>


<dt id="arg-metrics">metrics<a class="anchor" aria-label="anchor" href="#arg-metrics"></a></dt>
<dd><p>character. A vector of the metric names to compute. If <code>NULL</code>, "auc" (area under the ROC curve), "tss" (true skill statistics), "accuracy", "F1" (F1 score), "precision", and "recall" are computed for ROC-based metrics while "rmse" (root mean squared error), "mae" (mean absolute error) and "r2" (pseudo R-squared) are computed for error-based metrics.</p></dd>


<dt id="arg-overall-roc-metrics">overall_roc_metrics<a class="anchor" aria-label="anchor" href="#arg-overall-roc-metrics"></a></dt>
<dd><p>character. A vector of a subset of ROC-based metrics to be used for the overall composite score (<code>TOT_ROC_SCORE</code>). Allowed options are "auc", "tss", "accuracy", and "F1". If <code>NULL</code>, the sensible default is "auc", "tss" and "accuracy".
This metric is useful when the objective is to obtain a rapid overview of the rank of multiple candidate models fitted to datasets via blocked cross-validation using multi-criteria assessment.</p></dd>


<dt id="arg-overall-error-metrics">overall_error_metrics<a class="anchor" aria-label="anchor" href="#arg-overall-error-metrics"></a></dt>
<dd><p>character. A vector of a subset of continuous outcome metrics to be used for the overall composite score (<code>TOT_ERROR_SCORE</code>). Allowed options are "rmse", "mae", and "r2".
If <code>NULL</code>, the default is "rmse" and "mae". In order to obtain an overall interpretable score, it is imperative to select metrics that have the same scale.</p></dd>


<dt id="arg-is-pred-rate">is_pred_rate<a class="anchor" aria-label="anchor" href="#arg-is-pred-rate"></a></dt>
<dd><p>logical. If <code>TRUE</code>, it indicates that the <code>expected_response</code> contains predictions at the intensity (per-unit-of-exposure) scale (typical for Bayesian models with offset from <code>inlabru</code>). If <code>FALSE</code>, it assumes predictions are at the original scale (e.g., counts). Default is <code>FALSE</code>.</p></dd>


<dt id="arg-exposure">exposure<a class="anchor" aria-label="anchor" href="#arg-exposure"></a></dt>
<dd><p>character. The column name in the <code>sf</code> objects that contains the exposure variable (offset). Only relevant for count (and sometimes presence-absence) data and must be standardized across all these types of datasets.
If <code>is_pred_rate</code> is <code>TRUE</code>, observed counts are rescaled by this exposure variable. Default is <code>NULL</code>.</p></dd>


<dt id="arg-seed">seed<a class="anchor" aria-label="anchor" href="#arg-seed"></a></dt>
<dd><p>integer. It sets the seed for random number generation, used for pseudo-absence sampling to ensure reproducibility. Default is 25.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional arguments to be passed on to internal functions, particularly <a href="https://rdrr.io/pkg/pROC/man/coords.html" class="external-link">coords</a> function.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>An object of class <code>ISDMmetrics</code>. It is named <code>list</code> containing all requested metrics. The names follow a consistent convention:</p><ul><li><p><code>"&lt;METRIC&gt;_&lt;DATASET_NAME&gt;"</code>: Individual metric score for each dataset.</p></li>
<li><p><code>"&lt;METRIC&gt;_Comp"</code>: The sample-size-weighted composite score for a given metric across all valid datasets.</p></li>
<li><p><code>"TOT_ROC_SCORE"</code>: The overall ROC-based composite score, averaged across the selected <code>overall_roc_metrics</code>.</p></li>
<li><p><code>"TOT_ERROR_SCORE"</code>: The overall error-based composite score, averaged across <code>overall_error_metrics</code>.</p></li>
</ul><p>The <code>ISDMmetrics</code> object is a list containing performance values for individual datasets and composite scores.
Use <code><a href="https://rspatial.github.io/terra/reference/as.data.frame.html" class="external-link">as.data.frame()</a></code> to flatten these for cross-validation summaries.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>The function handles three main data types and any combination thereof:</p>
<ul><li><p><strong>Presence-Absence (PA) Data:</strong> The function uses the <code>response_pa</code> column and <code>prob_raster</code> to calculate all ROC-based metrics (see <a href="https://rdrr.io/pkg/pROC/man/coords.html" class="external-link">coords</a>, for more details on available metrics).</p></li>
<li><p><strong>Count Data (or optionally measurements):</strong> The function uses <code>expected_response</code> to calculate continuous-outcome metrics and can optionally use <code>prob_raster</code> to calculate ROC-based metrics for count data.</p></li>
<li><p><strong>Presence-Only (PO) Data:</strong> The function uses the presence points from the <code>sf</code> object (<code>xy_excluded</code>) and samples <code>n</code> pseudo-absence points from the study background (excluding <code>xy_excluded</code>) to create a presence-absence dataset for ROC-based metric calculations.</p></li>
</ul><p>For models based on count data, if a user wants to compute both continuous-outcome and ROC-based metrics, <code>expected_response</code> raster must be supplied for the continuous metrics and <code>prob_raster</code> must also be supplied for the ROC-based metrics.
The <code>prob_raster</code> can be obtained by converting the continuous-outcome prediction (e.g., <code>linear predictor</code>) to a suitability index using the <a href="suitability_index.html">suitability_index</a> function.</p>
<p>The available continuous-outcome metrics are given as follows:</p><ul><li><p><strong>Root Mean Squared Error (RMSE)</strong>: A measure of the average magnitude of the errors. It's the square root of the average of squared differences between prediction and actual observation. It gives higher weight to large errors.
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2}$$.</p></li>
<li><p><strong>Mean Absolute Error (MAE)</strong>: A measure of the average magnitude of the errors without considering their direction. It is the average of the absolute differences between prediction and actual observation.
$$MAE = \frac{1}{n}\sum_{i=1}^{n}|\hat{y_i} - y_i|$$.</p></li>
<li><p><strong>Mean Absolute Percentage Error (MAPE)</strong>: A measure of prediction accuracy as a percentage. It is calculated as the average of the absolute percentage errors for each observation. It can be useful for comparing performance across different datasets or models.
$$MAPE = \frac{100\%}{n}\sum_{i=1}^{n}|\frac{\hat{y_i} - y_i}{y_i}|$$.</p></li>
<li><p><strong>Pseudo R-squared (\(R^2\))</strong>: A measure of the proportion of variance in the observed data explained by the model's predictions.
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$
Where:</p><ul><li><p>\(y_i\) is the observed continuous value at location \(i\).</p></li>
<li><p>\(\hat{y}_i\) is the predicted value from the model at location \(i\) (e.g., the posterior mean of the predictions).</p></li>
<li><p>\(\bar{y}\) is the mean of all observed values.</p></li>
<li><p>\(SS_{res}\) is the residual sum of squares, which measures the discrepancy between the observed and predicted values:
$$SS_{res} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$</p></li>
<li><p>\(SS_{tot}\) is the total sum of squares, which measures the total variance in the observed data:
$$SS_{tot} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$</p></li>
</ul></li>
</ul><p>A <code>weighted composite score</code> (<code>&lt;METRIC&gt;_Comp</code>) is computed for each requested metric by taking the sample-size-weighted average across all datasets where the metric was successfully calculated.
A <code>total composite score</code> (<code>TOT_ROC_SCORE</code> or <code>TOT_ERROR_SCORE</code>) is also computed by averaging the selected metrics in the corresponding 'overall metrics' character vector.
It can be viewed as a quick <em>multi-criterion decision metric</em> for multiple models comparison.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="DataFolds-methods.html">extract_fold</a></code>, <code><a href="sample_background.html">sample_background</a></code></p>
<p>Other ISDM evaluation methods:
<code><a href="ISDMmetrics-methods.html">ISDMmetrics-methods</a></code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Assuming you have dummy prediction rasters and a list of sf objects</span></span></span>
<span class="r-in"><span><span class="co"># with 'counts' and 'present' columns for counts and PA data, respectively.</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example 1: Compute metrics for a presence-absence model</span></span></span>
<span class="r-in"><span><span class="co"># pa_metrics &lt;- compute_metrics(</span></span></span>
<span class="r-in"><span><span class="co">#   test_data = list(ds1 = my_pa_sf),</span></span></span>
<span class="r-in"><span><span class="co">#   prob_raster = prob_raster,  # compulsory prob_raster</span></span></span>
<span class="r-in"><span><span class="co">#   response_pa = "present"     # default labels column for all PA data</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example 2: Compute continuous-outcome metrics for a count-based model</span></span></span>
<span class="r-in"><span><span class="co"># cont_metrics &lt;- compute_metrics(</span></span></span>
<span class="r-in"><span><span class="co">#   test_data = list(ds1 = my_count_sf),</span></span></span>
<span class="r-in"><span><span class="co">#   expected_response = expected_raster, # prediction on count scale</span></span></span>
<span class="r-in"><span><span class="co">#   response_count = "counts",           # default labels column for all counts</span></span></span>
<span class="r-in"><span><span class="co">#   metrics = c("rmse", "mae", "mape")</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example 3: Compute both continuous and ROC-based metrics for a count model</span></span></span>
<span class="r-in"><span><span class="co"># The user must first generate a suitability index (prob_raster &amp; expected_response)</span></span></span>
<span class="r-in"><span><span class="co"># from the linear scale prediction (pred_eta).</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># expected_raster &lt;- suitability_index(pred_eta,</span></span></span>
<span class="r-in"><span><span class="co">#                    response_type = "count",</span></span></span>
<span class="r-in"><span><span class="co">#                    output_format = "response")</span></span></span>
<span class="r-in"><span><span class="co"># suitability_raster &lt;- suitability_index(pred_eta,</span></span></span>
<span class="r-in"><span><span class="co">#                       response_type = "count",</span></span></span>
<span class="r-in"><span><span class="co">#                       output_format = "prob")</span></span></span>
<span class="r-in"><span><span class="co"># full_metrics &lt;- compute_metrics(</span></span></span>
<span class="r-in"><span><span class="co">#   test_data = list(ds1 = my_count_sf),</span></span></span>
<span class="r-in"><span><span class="co">#   prob_raster = suitability_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   expected_response = expected_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   metrics = c("rmse", "mae", "auc", "tss")</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example 4: Handle an inlabru-like model with an offset term</span></span></span>
<span class="r-in"><span><span class="co"># The `expected_response` raster is at the intensity scale (rate).</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># cont_metrics &lt;- compute_metrics(</span></span></span>
<span class="r-in"><span><span class="co">#   test_data = list(ds1 = my_count_sf),</span></span></span>
<span class="r-in"><span><span class="co">#   prob_raster = suitability_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   expected_response = expected_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   metrics = c("rmse", "auc", "tss"),</span></span></span>
<span class="r-in"><span><span class="co">#   is_pred_rate = TRUE,</span></span></span>
<span class="r-in"><span><span class="co">#   exposure = "exposure_col" # Exposure column (e.g. sampling unit area)</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example 5: Compute dataset-specific and weighted composite metrics for a joint model</span></span></span>
<span class="r-in"><span><span class="co"># expected_raster  &lt;- suitability_index(pred_eta,</span></span></span>
<span class="r-in"><span><span class="co">#                     response_type = "count.pa",</span></span></span>
<span class="r-in"><span><span class="co">#                     output_format = "response")</span></span></span>
<span class="r-in"><span><span class="co"># suitability_raster &lt;- suitability_index(pred_eta,</span></span></span>
<span class="r-in"><span><span class="co">#                       response_type = "count.pa",</span></span></span>
<span class="r-in"><span><span class="co">#                       has_offset = FALSE)</span></span></span>
<span class="r-in"><span><span class="co"># full_metrics &lt;- compute_metrics(</span></span></span>
<span class="r-in"><span><span class="co">#   test_data = list(ds1 = my_count_sf, ds2 = my_pa_sf),</span></span></span>
<span class="r-in"><span><span class="co">#   prob_raster = suitability_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   expected_response = expected_raster,</span></span></span>
<span class="r-in"><span><span class="co">#   metrics = c("rmse", "mae", "auc", "tss", "accuracy")</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Akoeugnigan Idelphonse SODE.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

